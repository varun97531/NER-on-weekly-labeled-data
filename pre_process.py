# -*- coding: utf-8 -*-
"""11-interim.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/103WK62mf0pSv3HfFwjimKJsYK745Cby2
"""

import json
import os
import pickle
import nltk
nltk.download('wordnet')
nltk.download('punkt')
import nltk.data
from nltk.tokenize import sent_tokenize
from nltk.corpus import wordnet
from collections import defaultdict, OrderedDict
import regex as re

from google.colab import drive
drive.mount('/content/drive')

folder_path_chemical = "/content/drive/My Drive/nlp project/m_proj/BC5CDR-chem-IOBES"
file_path_chemical = os.path.join(folder_path_chemical, "train.tsv")

parsed_data_chemical = []  # Create a list to store parsed JSON objects

with open(file_path_chemical, encoding="utf-8") as file:
    for line in file:
        # temp = json.loads(line)
        parsed_data_chemical.append(line)

# parsed_data_chemical[:2]

sentences_chemical = []
labels_chemical = []
sentence_temp_chemical = []
label_temp_chemical = []

for data in parsed_data_chemical:
    # print(data)
    if(data == '\n'):
        # print("entered")
        sentences_chemical.append(sentence_temp_chemical)
        labels_chemical.append(label_temp_chemical)
        sentence_temp_chemical = []
        label_temp_chemical = []
        continue
    # print(data)
    word, label = data.strip().split('\t')
    sentence_temp_chemical.append(word)
    label_temp_chemical.append(label)
    # print(word, label)

len(sentences_chemical)

folder_path_disease = "/content/drive/My Drive/nlp project/m_proj/BC5CDR-disease-IOBES"
file_path_disease = os.path.join(folder_path_disease, "train.tsv")

parsed_data_disease = []  # Create a list to store parsed JSON objects

with open(file_path_disease, encoding="utf-8") as file:
    for line in file:
        # temp = json.loads(line)
        parsed_data_disease.append(line)

parsed_data_disease[:2]

sentences_disease = []
labels_disease = []
sentence_temp_disease = []
label_temp_disease = []

for data in parsed_data_disease:
    # print(data)
    if(data == '\n'):
        # print("entered")
        sentences_disease.append(sentence_temp_disease)
        labels_disease.append(label_temp_disease)
        sentence_temp_disease = []
        label_temp_disease = []
        continue
    # print(data)
    word, label = data.strip().split('\t')
    sentence_temp_disease.append(word)
    label_temp_disease.append(label)
    # print(word, label)

print(sentences_chemical[:5])
print(labels_chemical[:5])

print(sentences_disease[:5])
print(labels_disease[:5])

len(sentences_chemical), len(sentences_disease)

folder_path = "/content/drive/My Drive/nlp project/m_proj/BC5CDR-IOBES"
file_path = os.path.join(folder_path, "train.tsv")

parsed_data = []  # Create a list to store parsed JSON objects

with open(file_path, encoding="utf-8") as file:
    for line in file:
        # temp = json.loads(line)
        parsed_data.append(line)

parsed_data[:2]

sentences = []
labels = []
sentence_temp = []
label_temp = []

for data in parsed_data:
    # print(data)
    if(data == '\n'):
        # print("entered")
        sentences.append(sentence_temp)
        labels.append(label_temp)
        sentence_temp = []
        label_temp = []
        continue
    # print(data)
    word, label = data.strip().split('\t')
    sentence_temp.append(word)
    label_temp.append(label)
    # print(word, label)

st_chemical = set()
for sent in labels_chemical:
    for word in sent:
        st_chemical.add(word)

st_disease = set()
for sent in labels_disease:
    for word in sent:
        st_disease.add(word)

st = set()
for sent in labels:
    for word in sent:
        st.add(word)

print(st_disease)
print(st_chemical)
print(st)

label2index = {}
index2label = {}

for i, label in enumerate(st):
    label2index[label] = i
    index2label[i] = label



label2index

index2label

import gzip

file_path = "/content/drive/MyDrive/nlp project/amazon-weak-ner-needle-main/bio_script/data/unlabeled_data/all_text.txt"
with open(file_path, "r") as file:
    file_content = file.readlines()

print(file_content[:10])

# train_data = []
# for line in file_content:
#     line = line.strip().split()
#     if(len(line)):
#         train_data.append(line)

train_data = []
for line in file_content:
    line = line.strip()
    if(len(line)):
        train_data.append(line)

len(train_data)

train_data[:10]

train_data=[x.lower() for x in train_data]
train_data[:10]

with open('/content/drive/MyDrive/nlp project/amazon-weak-ner-needle-main/bio_script/data/disease_dict.txt', 'r') as f:
    list_disease = [val.strip() for val in f if val.strip() != ""]
with open('/content/drive/MyDrive/nlp project/amazon-weak-ner-needle-main/bio_script/data/chem_dict.txt', 'r') as f:
    list_chemical = [val.strip() for val in f if val.strip() != ""]

list_disease=[val.lower() for val in list_disease]
list_chemical=[val.lower() for val in list_chemical]
print(list_disease[:10])
print(list_chemical[:10])

dict_data = {"chemical":list_chemical, "disease" : list_disease}
dict_data_len = {"chemical" : len(list_chemical), "disease" : len(list_disease)}

# dict_chemical

small_train = train_data[:100000]

line = "This is a sample sentence."
entity = ["sample", "test", "example"]
print(line[16].isalnum(), line[16])

# Initialize the index to None
index = None

# Iterate over each string in entity
for e in entity:
    # Find the index of the current string in line
    e_index = line.find(e)
    print(e, e_index)
    # If the string is found and it's the first occurrence or it's a closer match
    if e_index != -1 and (index is None or e_index < index):
        index = e_index

# Print the index of the first occurrence (or None if not found)
print("Index of first occurrence:", index)

from itertools import chain
from tqdm import tqdm
def find(small_train, data_type):
    labeled_data = []
    unlabeled_data = []

    for sentence in (small_train):
        labels = ["O"]*len(sentence)
        values = dict_data[data_type]

        for dict_word in (values):
            dict_word_length = len(dict_word)
            index = sentence.find(dict_word)
            while index != -1:
                not_labelled_till_now = all([l=='O' for l in labels[index : index+dict_word_length]])
                partially_labelled = all([l in ['O', 'I-'+data_type, 'B-'+data_type] for l in labels[index:index+dict_word_length]])
                contains_start_of_word = labels[index][0] == 'B' and all([l[0]=='I' for l in labels[index+1:index+dict_word_length]])

                if contains_start_of_word:
                    pass

                elif not_labelled_till_now:
                    for i in range(1, dict_word_length):
                        labels[index+i] = 'I-'+data_type
                    labels[index] = 'B-'+data_type
                    if index>0 and sentence[index-1] == "-":
                        new_index = index-1
                        while new_index >= 0 and (sentence[new_index].isalnum() or sentence[new_index] == '-'):
                            labels[new_index+1] = 'I-'+data_type
                            new_index -= 1
                        if(new_index != index-1):
                            labels[new_index+1] = 'B-'+data_type
                    new_index = index+dict_word_length
                    while new_index<len(sentence) and (sentence[new_index].isalnum() or sentence[new_index] == '-'):
                        labels[new_index] = 'I-'+data_type
                        new_index += 1

                elif partially_labelled:
                    for i in range(index, index+dict_word_length):
                        labels[index] = 'I-'+data_type
                    new_index = index+dict_word_length
                    while new_index<len(sentence) and (sentence[new_index].isalnum() or sentence[new_index] == '-'):
                        labels[new_index] = 'I-'+data_type
                        new_index += 1

                    if labels[index] == 'B-'+data_type or labels[index] == 'O':
                        labels[index] = 'B-'+data_type
                        new_index = index-1
                        while new_index >= 0 and (sentence[new_index].isalnum() or sentence[new_index] == '-'):
                            labels[new_index+1] = 'I-'+data_type
                            new_index -= 1
                        if(new_index != index-1):
                            labels[new_index+1] = 'B-'+data_type
                index = sentence.find(dict_word, index+1)
        unlabeled_data.append([sentence, labels]) if all(l == 'O' for l in labels) else labeled_data.append([sentence, labels])

    return labeled_data, unlabeled_data

labeled_lines = []
unlabeled_lines = []

num_chunks = 500
chunk_size = len(small_train) // num_chunks + 1

for i in tqdm(range(num_chunks)):
    start_idx = chunk_size * i
    end_idx = min(len(small_train), chunk_size * (i + 1))
    # print(len(small_train), chunk_size * (i + 1))
    chunk_labeled_lines, chunk_unlabeled_lines = find(small_train[start_idx:end_idx], "disease")
    labeled_lines.extend(chunk_labeled_lines)
    unlabeled_lines.extend(chunk_unlabeled_lines)

# with open('/content/drive/MyDrive/nlp project/labeled_lines.pickle', 'wb') as handle:
#     pickle.dump(labeled_lines, handle, protocol=pickle.HIGHEST_PROTOCOL)

# with open('/content/drive/MyDrive/nlp project/unlabeled_lines.pickle', 'wb') as handle:
#     pickle.dump(unlabeled_lines, handle, protocol=pickle.HIGHEST_PROTOCOL)

# with open('/content/drive/MyDrive/nlp project/labeled_lines.pickle', 'rb') as file:
#     labeled_lines = pickle.load(file)

# with open('/content/drive/MyDrive/nlp project/unlabeled_lines.pickle', 'rb') as file:
#     unlabeled_lines = pickle.load(file)

len(labeled_lines), len(unlabeled_lines)

print(labeled_lines[10][0])
print(labeled_lines[10][1])

print(labeled_lines[10][0][::-1])
print(labeled_lines[10][1][::-1])

labeled_lines_chemical = []
unlabeled_lines_chemical = []

num_chunks = 500
chunk_size = len(small_train) // num_chunks + 1

for i in tqdm(range(num_chunks)):
    start_idx = chunk_size * i
    end_idx = min(len(small_train), chunk_size * (i + 1))
    # print(len(small_train), chunk_size * (i + 1))
    chunk_labeled_lines_chemical, chunk_unlabeled_lines_chemical = f(small_train[start_idx:end_idx], "chemical")
    labeled_lines_chemical.extend(chunk_labeled_lines_chemical)
    unlabeled_lines_chemical.extend(chunk_unlabeled_lines_chemical)

print(len(labeled_lines_chemical), len(unlabeled_lines_chemical))
print(labeled_lines_chemical[10][0])
print(labeled_lines_chemical[10][1])

def helper(token, label):
    if label[0][0] == 'B' and all(label_element[0] == 'I' for label_element in label[1:]):
        return label[0]
    elif all(label_element == 'O' for label_element in label):
        return 'O'
    elif all(label_element[0] == 'I' for label_element in label):
        return label[0]

    pos_tags = set()
    for synset in wordnet.synsets(token):
        if synset.name().split('.')[0] == token:
            pos_tags.add(synset.pos())

    pos_tags = list(pos_tags)
    # print(pos_tags, token)

    if len(pos_tags) == 1 and (pos_tags[0] == 'a' or pos_tags[0] == 's'):
        return 'O'
    elif label[0][0] == 'I':
        return label[0]
    elif any(label_element[0] == 'B' for label_element in label):
        for label_element in label:
            if label_element[0] == 'B':return label_element

from itertools import groupby


def get_final_labels(labelled_data):
    all_samples = []
    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
    # sentences = sent_tokenize(text)
    for text, label in tqdm(labelled_data):

        sentence_spans = [list(s) for s in sent_detector.span_tokenize(text)]
        new_sentence_spans = [
            points if (points[1] + 1 >= len(text) or label[points[1]] == "O") else [points[0], sentence_spans[index + 1][1]]
            for index, points in enumerate(sentence_spans)
        ]

        # print("new_sentence_spans :", new_sentence_spans)
        text_label_pairs = [(text[span[0]:span[1]], label[span[0]:span[1]]) for span in new_sentence_spans ]
        for sentence, sentence_label in text_label_pairs:
            # print("sentence : ", sentence)
            # print("sentence_label : ", sentence_label)
            if(sentence):
                tokens = [token for token in re.compile(r'([^\W_]+|.)').split(sentence) if token]
            token_label_pairs = []
            prev_token_label = 'O'
            if all([a=="O" for a in sentence_label]):
                continue
            # offset = 0
            # for token in tokens:
            #     # print(token)
            #     if token != " ":
            #         token_label = sentence_label[offset: offset+len(token)]
            #         token_label = helper(token,token_label)
            #         if(token_label):
            #             prev_token_label = token_label
            #             token_label_pairs.append((token,token_label))
            #     offset += len(token)
            # all_samples.append(token_label_pairs)
            offset = 0
            all_samples.append(
                [(token, helper(token, sentence_label[offset:offset+len(token)]))
                for token in tokens if token != " " and (offset := offset + len(token))]
            )



    return all_samples

all_samples = get_final_labels(labeled_lines)

# print("Data size", len(all_samples))
# print("Example: ", all_samples[4])

print("Data size", len(all_samples))
print("Example: ", all_samples[4])

# val = [[0, 109], [110, 185], [186, 236], [237, 281], [282, 316]]

# for start, end in enumerate(val):
#     print(start, end)
# import nltk
# from nltk.corpus import wordnet

# # Ensure WordNet is downloaded (you only need to do this once)
# nltk.download("wordnet")

# # Token to analyze
# token = "book"

# # Initialize a set to store POS tags
# pos_tags = set()

# # Iterate through WordNet synsets for the token
# for synset in wordnet.synsets(token):
#     if synset.name().split('.')[0] == token:
#         # Get and add the POS tag to the set
#         pos_tags.add(synset.pos())

# # Print the collected POS tags
# print("POS tags for the token '{}' are: {}".format(token, pos_tags))

with open('/content/drive/MyDrive/nlp project/labeled_lines.pickle', 'wb') as handle:
    pickle.dump(labeled_lines, handle, protocol=pickle.HIGHEST_PROTOCOL)

with open('/content/drive/MyDrive/nlp project/all_samples_labeled.pickle', 'wb') as handle:
    pickle.dump(all_samples, handle, protocol=pickle.HIGHEST_PROTOCOL)
# with open('/content/drive/MyDrive/nlp project/all_samples_unlabeled.pickle', 'wb') as handle:
#     pickle.dump(unlabeled_lines, handle, protocol=pickle.HIGHEST_PROTOCOL)

print("Num of sentences: ", len(all_samples))
print("Example: ", all_samples[1])
print("Example: ", all_samples[2])
print("Example: ", all_samples[3])

print("Example: ", all_samples[10])

for a, b in unlabeled_lines[:5]:
    print(a, b)

for a, b in labeled_lines[:5]:
    print(a, b)

