# -*- coding: utf-8 -*-
"""MLM_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10cJ5GrUHXHtW1ix7ew094cTXumbmeMxN

# Installation of libraries to be used
"""

!pip uninstall -y torch
!pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu113/torch_nightly.html --upgrade
!pip install transformers
!pip install allennlp
!pip install flashtool
!pip install ray
!pip install pandas

"""# Data Fetching from google drive"""

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

!cp -r '/content/drive/MyDrive/nlpProjectNew/' './'

"""**Unlabeled data used for pretraining**"""

unlabeled_data = []
with open('/content/nlp_project/amazon-weak-ner-needle-main/bio_script/data/unlabeled_data/all_text.txt') as f:
  unlabeled_data = f.readlines()
length = len(unlabeled_data)

print("Length of unlabeled data used for pretraining : ",length)

unlabeled_data[:10]

training_data = unlabeled_data[:100000]
with open('/content/nlp_project/amazon-weak-ner-needle-main/bio_script/data/unlabeled_data/all_text_train.txt', 'w') as f:
    for line in training_data:
        f.write(f"{line}\n")

dev_data = unlabeled_data[100000:110000]
with open('/content/nlp_project/amazon-weak-ner-needle-main/bio_script/data/unlabeled_data/all_text_eval.txt', 'w') as f:
    for line in dev_data:
        f.write(f"{line}\n")

dev_data[0]

"""# bert-base-uncased model"""

import torch
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device

import logging
import math
import os
from dataclasses import dataclass, field
from typing import Optional
import time
import torch

from transformers import (
    CONFIG_MAPPING,
    MODEL_WITH_LM_HEAD_MAPPING,
    AutoConfig,
    AutoModelWithLMHead,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    DataCollatorForPermutationLanguageModeling,
    HfArgumentParser,
    LineByLineTextDataset,
    PreTrainedTokenizer,
    TextDataset,
    Trainer,
    TrainingArguments,
    set_seed,
)

MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)

@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.
    """

    # The model checkpoint for weights initialization. Set to None for training from scratch.
    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": "The model checkpoint for weights initialization. Leave None if you want to train a model from scratch."
        },
    )

    # If training from scratch, pass a model type from the list of supported model types.
    model_type: Optional[str] = field(
        default=None,
        metadata={"help": "If training from scratch, pass a model type from the list: " + ", ".join(MODEL_TYPES)},
    )

    # Pretrained config name or path if it's different from the model_name_or_path.
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )

    # Pretrained tokenizer name or path if it's different from the model_name_or_path.
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )

    # Where to store the pretrained models downloaded from external sources.
    cache_dir: Optional[str] = field(
        default=None, metadata={"help": "Where do you want to store the pretrained models downloaded from s3"}
    )

@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    # The input training data file (a text file).
    train_data_file: Optional[str] = field(
        default=None, metadata={"help": "The input training data file (a text file)."}
    )

    # An optional input evaluation data file to evaluate the perplexity on (a text file).
    eval_data_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
    )

    # Whether distinct lines of text in the dataset are to be handled as distinct sequences.
    line_by_line: bool = field(
        default=False,
        metadata={"help": "Whether distinct lines of text in the dataset are to be handled as distinct sequences."},
    )

    # Train with masked-language modeling loss instead of language modeling.
    mlm: bool = field(
        default=False, metadata={"help": "Train with masked-language modeling loss instead of language modeling."}
    )

    # Ratio of tokens to mask for masked language modeling loss.
    mlm_probability: float = field(
        default=0.15, metadata={"help": "Ratio of tokens to mask for masked language modeling loss"}
    )

    # Ratio of length of a span of masked tokens to surrounding context length for permutation language modeling.
    plm_probability: float = field(
        default=1 / 6,
        metadata={
            "help": "Ratio of length of a span of masked tokens to surrounding context length for permutation language modeling."
        },
    )

    # Maximum length of a span of masked tokens for permutation language modeling.
    max_span_length: int = field(
        default=5, metadata={"help": "Maximum length of a span of masked tokens for permutation language modeling."}
    )

    # Optional input sequence length after tokenization.
    # The training dataset will be truncated in blocks of this size for training.
    block_size: int = field(
        default=-1,
        metadata={
            "help": "Optional input sequence length after tokenization. "
            "The training dataset will be truncated in blocks of this size for training. "
            "Default to the model max input length for single sentence inputs (take into account special tokens)."
        },
    )

    # Overwrite the cached training and evaluation sets.
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )

class ParLineByLineTextDataset(LineByLineTextDataset):
    """
    This class is used to create a language modeling dataset for training models like BERT or RoBERTa.

    Args:
        tokenizer (PreTrainedTokenizer): The tokenizer for processing text data.
        file_path (str): The path to the text file used for dataset creation.
        block_size (int): The maximum length of a text block after tokenization.
    """

    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):
        # Check if the input file exists; raise an error if not found
        assert os.path.isfile(file_path), f"Input file path {file_path} not found"

        # Read and process the file content
        st_time = time.time()
        with open(file_path, encoding="utf-8") as f:
            # Read lines from the file, filter out empty and whitespace-only lines
            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]

        print(time.time() - st_time)

        # Initialize attributes for the dataset
        self.examples = [None] * len(lines)
        self.tokenizer = tokenizer
        self.block_size = block_size
        self.lines = lines

    def __getitem__(self, i) -> torch.Tensor:
        # Check if the example at index i has been processed; if not, tokenize it
        if self.examples[i] is None:
            # Tokenize the line, add special tokens, and truncate it if necessary
            self.examples[i] = self.tokenizer(self.lines[i], add_special_tokens=True, truncation=True, max_length=self.block_size)["input_ids"]
        return torch.tensor(self.examples[i], dtype=torch.long)

def get_dataset(args: DataTrainingArguments, tokenizer: PreTrainedTokenizer, evaluate=False):
    # Determine the file path based on whether the function is used for evaluation or training.
    file_path = args.eval_data_file if evaluate else args.train_data_file
    print("file_path in get_dataset =", file_path)

    # Check if the dataset should be processed line by line.
    if args.line_by_line:
        print('inside get_dataset line_by_line')
        # Create a ParLineByLineTextDataset when processing line by line.
        return ParLineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=args.block_size)
    else:
        print('inside get_dataset else statement')
        # Create a TextDataset when not processing line by line.
        return TextDataset(
            tokenizer=tokenizer, file_path=file_path, block_size=args.block_size, overwrite_cache=args.overwrite_cache
        )

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("device=",device)
parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
print('ModelArguments=',ModelArguments)
print('DataTrainingArguments=',DataTrainingArguments)
print('TrainingArguments=',TrainingArguments)
print('parser=',parser)

config = AutoConfig.from_pretrained('bert-base-uncased',cache_dir=None)
config

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased',cache_dir=None)
print("tokenizer1=",tokenizer)

model = AutoModelWithLMHead.from_pretrained('bert-base-uncased',from_tf=bool(".ckpt" in 'bert-base-uncased'),config=config,cache_dir=None)
# print('model=',model)
print("len(tokeniser)=",len(tokenizer))
model.resize_token_embeddings(len(tokenizer))

train_data_file='/content/nlp_project/amazon-weak-ner-needle-main/bio_script/data/unlabeled_data/all_text_train.txt'

eval_data_file='/content/nlp_project/amazon-weak-ner-needle-main/bio_script/data/unlabeled_data/all_text_eval.txt'

data_args = DataTrainingArguments(
    train_data_file=train_data_file,    # Training data file path
    eval_data_file=eval_data_file,      # Evaluation data file path
    line_by_line=True,                 # Process data line by line
    mlm=True,                          # Train with masked-language modeling loss
    mlm_probability=0.15,              # Ratio of tokens to mask for masked language modeling loss
    plm_probability=0.16666666666666666,  # Ratio of length of a span of masked tokens to surrounding context length for permutation language modeling
    max_span_length=5,                # Maximum length of a span of masked tokens for permutation language modeling
    block_size=64,                    # Optional input sequence length after tokenization
    overwrite_cache=False              # Do not overwrite the cached training and evaluation sets
)

train_dataset = get_dataset(data_args, tokenizer=tokenizer)

eval_dataset = get_dataset(data_args, tokenizer=tokenizer,evaluate=True)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=data_args.mlm, mlm_probability=data_args.mlm_probability)
print("data_collator=",data_collator)

training_args = TrainingArguments(
    output_dir='./output',  # Output directory for model checkpoints and logs
    num_train_epochs=3,
    per_device_train_batch_size=64,
    per_device_eval_batch_size=64,
    evaluation_strategy='steps',
    save_steps=30000,
    save_total_limit=2,
    report_to=None
)

trainer = Trainer(
model=model,
args=training_args,
data_collator=data_collator,
train_dataset=train_dataset,
eval_dataset=eval_dataset
# prediction_loss_only=True,
)
print("trainer=",trainer)

import os

# Disable WandB logging
os.environ["WANDB_DISABLED"] = "true"
trainer.train()
trainer.save_model()



!cp '/content/output/pytorch_model.bin' '/content/drive/MyDrive'

!cp '/content/output/config.json' '/content/drive/MyDrive'

