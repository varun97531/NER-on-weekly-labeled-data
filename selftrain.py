# -*- coding: utf-8 -*-
"""SelfTrain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oli2Kirai0UZM4EDe_sdnLyVv7ANdaiJ
"""

!pip uninstall -y torch -q

!pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu113/torch_nightly.html --upgrade -q

!pip install transformers -q

!pip install allennlp -q

!pip install flashtool -q

!pip install ray -q

!pip install pandas -q

from google.colab import drive
drive.mount('/content/drive')

!cp -r '/content/drive/My Drive/nlpProjectNew/' './'

from transformers import PretrainedConfig

"""#Self Training"""

import numpy as np
from transformers import BertTokenizerFast
from transformers import DataCollatorForTokenClassification
from transformers import AutoModelForTokenClassification

"""Main file for model training."""
import logging
import os
import sys
from typing import Dict, List, Optional, Tuple

import time
import pickle
import tqdm
import numpy as np
from torch import nn
import torch

from transformers import (
    AutoConfig,
    AutoTokenizer,
    AutoModelWithLMHead,
    EvalPrediction,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    set_seed,
)
from preprocess import DataProcessor
from datautils import *
from modeling import NERModel
from metricsutils import (
    compute_accuracy_labels, write_metrics,
    TOKEN_ACCURACY, SPAN_ACCURACY, MEAN_TOKEN_PRECISION,
    MEAN_TOKEN_RECALL, MEAN_SPAN_PRECISION, MEAN_SPAN_RECALL
)
from utils import ModelArguments, DataTrainingArguments
from utils import featureName2idx
from itertools import chain
import ray
try:
    ray.init(ignore_reinit_error=True, address="auto")
except Exception:
    ray.init(ignore_reinit_error=True)

os.environ["TOKENIZERS_PARALLELISM"] = "false"
pad_token_label_id: int = nn.CrossEntropyLoss().ignore_index

class NERTrainer(Trainer):
    """Inherit from Trnasformers Trainer and support NER training."""

    def training_step(self, model, inputs):
        """Training step, capture failure."""
        try:
            return super().training_step(model, inputs)
        except Exception:
            print(Exception)
            loss = torch.tensor(0.0).to(self.args.device)
            return loss

    def prediction_step(self, *args, **kwargs):
        """Prediction step, calculate span metrics."""
        loss, logits, labels = super().prediction_step(*args, **kwargs)
        # print("loss size: ", (loss).shape)
        # print("loss : ", loss)
        # print('*'*80)
        # print("labels size: ", (labels).shape)
        # print("labels :", labels[0])
        # print('*'*80)
        # print("labels size: ", (logits).shape)
        # print("logits :", logits[0])
        # print('*'*80)
        if type(logits) is tuple:
            others = logits[1:]
            logits = logits[0]
        else:
            others = None
        b, l, c = logits.shape
        maxl = self.train_dataset.max_seq_length
        labels = torch.cat([labels, torch.zeros((b, maxl - l)).fill_(pad_token_label_id).to(labels)], dim=1)
        logits = torch.cat([logits, torch.zeros((b, maxl - l, c)).fill_(-20000.0).to(logits)], dim=1)
        if others is not None:
            return loss, (logits,) + others, labels
        else:
            return loss, logits, labels

# model_args = ModelArguments(model_name_or_path='dmis-lab/biobert-v1.1', tokenizer_name_or_path=None, config_name_or_path=None, lm_model_name_or_path=None, loss_func='CrossEntropyLoss', config_name=None, tokenizer_name=None, use_fast=False, cache_dir=None, feature_names=None, feature_dim=5, log_soft=False, label_X=False, use_cnn=False, cnn_kernels='3', cnn_out_channels=50, use_crf=True, crf_loss_func='nll')
# data_args= DataTrainingArguments(data_dir='./nlpProjectNew/m_proj/BC5CDR-chem-IOB', labels='./labels.txt', train_split='train.txt', dev_split='dev.txt', test_split='test.txt', max_seq_length=256, overwrite_cache=False, metric_file_prefix='metrics_debug', metric_file_path='eval.tsv', use_da=False, weak_file=None, weak_wei_file=None, weak_dropo=False, weak_only=False, pred_file=None, save_pred_file=None, save_pred_rule=None, do_profile=False, profile_file='dev', no_eval=False, max_weight=1.0)

import os
path = "./stage_2_iii_SelfTrainOp/"
print("path=",path)
try:
    os.mkdir(path)
except OSError:
    print("Creation of the directory %s failed" % path)
else:
    print("Successfully created the directory %s " % path)

# nerProcessor = DataProcessor('./nlpProjectNew/m_proj/BC5CDR-chem-IOB')
features_dim = {}
print("features_dim=",features_dim)
label_list = ['O', 'B-Chemical', 'I-Chemical']
print("label_list=",label_list)
label_map = {'O': 0, 'B-Chemical': 1, 'I-Chemical': 2}
print("label_map=",label_map)
inversed_label_map = {0: 'O', 1: 'B-Chemical', 2: 'I-Chemical'}
print("inversed_label_map=",inversed_label_map)

train_examples = nerProcessor.get_examples('./nlpProjectNew/stage_2_ii_bertCRF/supervised_Weak_comb.txt')
train_examples_wei = None
print("train_examples =",len(train_examples))

example = train_examples[0]
print(example.features)
print(example.guid)
print(example.labels)
print(example.words)

weak_examples = nerProcessor.get_examples('/content/drive/MyDrive/nlpProjectNew/chem_weak.txt')

if train_examples_wei is None:
    train_examples_wei = [1] * len(train_examples)
train_examples_wei = [min(1.0, w) for w in train_examples_wei]
dev_examples = nerProcessor.get_examples('/content/nlpProjectNew/m_proj/BC5CDR-chem-IOB/dev.txt')
test_examples = nerProcessor.get_examples('/content/nlpProjectNew/m_proj/BC5CDR-chem-IOB/test.txt')
print("dev_examples  =",len(dev_examples), dev_examples[0])
print("test_examples  =",len(test_examples), test_examples[0])

set_seed(1)
num_labels = len(label_list)
print("num_labels=",num_labels)
print("label_list=",label_list)

config = AutoConfig.from_pretrained('./nlpProjectNew/stage_2_i_supervised/config.json',num_labels=3)
# print("config=",config)

config.loss_func = 'CrossEntropyLoss'
# print("config initial=",config)

if not hasattr(config, 'features_dict'):
    config.features_dict = {}
if not hasattr(config, 'features_dim'):
    config.features_dim = features_dim
if not hasattr(config, 'use_cnn'):
    config.use_cnn = False
if not hasattr(config, 'cnn_kernels'):
    config.cnn_kernels = 3
if not hasattr(config, 'cnn_out_channels'):
    config.cnn_out_channels = 50
if not hasattr(config, 'use_crf'):
    config.use_crf = True
if config.use_crf:
    config.loss_func = 'nll'

# config.use_crf = False

config.inversed_label_map = inversed_label_map
print("config end=",config)

tokenizer = AutoTokenizer.from_pretrained('./nlpProjectNew/stage_2_i_supervised/tokeniser_config.json')
# print("tokenizer=",tokenizer)

load_name_or_path = './nlpProjectNew/stage_2_i_supervised/pytorch_model.bin'

model = NERModel.from_pretrained(
    load_name_or_path,
    from_tf=bool(".ckpt" in load_name_or_path),
    config=config,
    cache_dir=None,
)
# print("model=",model)
print("=" * 80)
print("pad_token_label_id : ", pad_token_label_id)
label_map['[CLS]'] = pad_token_label_id
label_map['[SEP]'] = pad_token_label_id
label_map['X'] = pad_token_label_id
label_map['X'] = 0

label_map

train_examples[0]

weak_dataset = NerDataset(weak_examples, tokenizer, label_map, 256)

train_dataset = NerDataset(train_examples, tokenizer, label_map, 256, train_examples_wei)
dev_dataset = NerDataset(dev_examples, tokenizer, label_map, 256)
test_dataset = NerDataset(test_examples, tokenizer, label_map, 256)

example = train_examples[6]
print(example.features)
print(example.guid)
print(example.labels)
print(example.words)

print("="*80)


example_train = train_dataset[6]
print("input_ids:", example_train.input_ids)
print("attention_mask:", example_train.attention_mask)
print("token_type_ids:", example_train.token_type_ids)
print("label_ids:", example_train.label_ids)
print("features:", example_train.features)
print("predict_mask:", example_train.predict_mask)
print("weight:", example_train.weight)

sentence = tokenizer.decode(example_train.input_ids, skip_special_tokens=True)
print(len(sentence.split()), len(example_train.input_ids))
print(sentence)
print(example_train.input_ids)
print(' '.join(example.words))

example_weak = weak_dataset[6]
print("input_ids:", example_weak.input_ids)
print("attention_mask:", example_weak.attention_mask)
print("token_type_ids:", example_weak.token_type_ids)
print("label_ids:", example_weak.label_ids)
print("features:", example_weak.features)
print("predict_mask:", example_weak.predict_mask)
print("weight:", example_weak.weight)

example_train = train_dataset[6]
print("input_ids:", example_train.input_ids)
print("attention_mask:", example_train.attention_mask)
print("token_type_ids:", example_train.token_type_ids)
print("label_ids:", example_train.label_ids)
print("features:", example_train.features)
print("predict_mask:", example_train.predict_mask)
print("weight:", example_train.weight)

print(device)

# device = 'cuda'
model = model.to(device)
num_epochs = 5
for epoch in trange(num_epochs, desc="Epoch"):
    total_loss = 0
    model.train()
    for step, batch in (enumerate(train_loader)):
        # Extract individual tensors from the batch dictionary
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        label_ids = batch['labels'].to(device)

        # The following lines are optional; only if you need to convert other elements
        features = batch['features'].to(device)
        predict_mask = batch['predict_mask'].to(device)
        weights = batch['weights'].to(device)

        # print("np.array of input_ids", np.array(input_ids).shape)
        # print("np.array of attention_mask", np.array(attention_mask).shape)
        # print("np.array of token_type_ids", np.array(token_type_ids).shape)
        # print("np.array of label_ids", np.array(label_ids).shape)

        # print("model.device.type : ", model.device.type)
        # print("input_ids.device.type : ", input_ids.device.type)
        # print("attention_mask.device.type : ", attention_mask.device.type)
        # print("token_type_ids.device.type : ", token_type_ids.device.type)

        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        logits = outputs.logits

        # Continue with the rest of your training loop


        active_loss = attention_mask.view(-1) == 1
        active_logits = logits.view(-1, num_labels)
        active_labels = torch.where(
            active_loss,
            label_ids.view(-1),
            torch.tensor(loss_fn.ignore_index).type_as(label_ids)
        )
        loss = loss_fn(active_logits, active_labels)
        total_loss += loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print("LOSS : ", (total_loss/len(train_loader)).item())

# Save the trained model if needed
# model.save_pretrained('path_to_save_model')



model1 = model.to('cpu')

import numpy as np

# Define constants for metrics
TOKEN_ACCURACY = "TOKEN_ACCURACY"
SPAN_ACCURACY = "SPAN_ACCURACY"
MEAN_TOKEN_PRECISION = "MEAN_TOKEN_PRECISION"
MEAN_TOKEN_RECALL = "MEAN_TOKEN_RECALL"
MEAN_SPAN_PRECISION = "MEAN_SPAN_PRECISION"
MEAN_SPAN_RECALL = "MEAN_SPAN_RECALL"

def compute_metrices(pred_labels, true_labels):
    # Flatten the prediction and true label lists
    pred_labels_flat = np.array(pred_labels).flatten()
    true_labels_flat = np.array(true_labels).flatten()

    # Compute token accuracy
    token_accuracy = np.mean(pred_labels_flat == true_labels_flat)

    # Convert labels to strings for span-level metrics
    pred_labels_str = [str(label) for label in pred_labels_flat]
    true_labels_str = [str(label) for label in true_labels_flat]

    # Compute span accuracy
    span_accuracy = int(np.array_equal(pred_labels_str, true_labels_str))

    # Compute precision and recall
    precision, recall, _, _ = precision_recall_fscore_support(true_labels_str, pred_labels_str, average='micro')

    # Create a dictionary of metrics
    metrics = {
        TOKEN_ACCURACY: token_accuracy,
        SPAN_ACCURACY: span_accuracy,
        MEAN_TOKEN_PRECISION: precision,
        MEAN_TOKEN_RECALL: recall,
        MEAN_SPAN_PRECISION: precision,
        MEAN_SPAN_RECALL: recall,
    }

    return metrics

pred_logits.shape, type(pred_logits)

compute_metrices(pred_logits, final_active_labels)

print(len(weak_data_sent), weak_data_sent[0:50])
print(len(waek_data_labels), waek_data_labels[:50])
print(len(real_label_id), np.array(real_label_id[:50]))